{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "I don't have information on specific attractions available at Jewel Changi Airport. However, I can tell you that Jewel Changi Airport is a shopping and entertainment complex located within Changi Airport in Singapore, and it features various amenities such as restaurants, shops, and entertainment options like the Rain Vortex and Crystal Pavillion.\n",
      "\n",
      "Sources Used:\n",
      "- Attractions: l\n",
      "- Attractions: b\n",
      "- Attractions: b\n",
      "- Attractions: e\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "\n",
    "# Paths to index and metadata files\n",
    "index_file = r\"D:\\Portfolio Github\\Airport_Chatbot\\data\\vector_index.faiss\"\n",
    "metadata_file = r\"D:\\Portfolio Github\\Airport_Chatbot\\data\\metadata.json\"\n",
    "\n",
    "# Load the metadata\n",
    "with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Step 1: Load FAISS Index\n",
    "\n",
    "# Initialize embeddings model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS index\n",
    "vector_store = FAISS.from_texts(texts=index_file, embedding=embedding_model, metadatas=metadata[:len(index_file)])\n",
    "\n",
    "# Save FAISS index\n",
    "vector_store.save_local(\"./data/index_folder\")\n",
    "\n",
    "# Load FAISS index into LangChain's vector store\n",
    "retriever = FAISS.load_local(\"./data/index_folder\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "# Step 2: Configure Ollama LLM\n",
    "llm = Ollama(model=\"llama3.2\", temperature = 0)  # Use the Ollama client with the chosen model\n",
    "\n",
    "# Step 3: Build the RAG Chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Step 4: Query the RAG System\n",
    "query = \"What attractions are available at Jewel Changi Airport?\"\n",
    "result = qa_chain({\"query\":query})\n",
    "\n",
    "# Display Results\n",
    "print(\"Generated Answer:\")\n",
    "print(result['result'])\n",
    "print(\"\\nSources Used:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"- {doc.metadata['category']}: {doc.page_content}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
